# Langfuse Integration Guide for AWS Bedrock AgentCore

This document provides a comprehensive guide on integrating Langfuse observability with Amazon Bedrock AgentCore and Strands Agents.

## Table of Contents

- [Overview](#overview)
- [Langfuse Hosting Options](#langfuse-hosting-options)
- [Architecture: How It Works](#architecture-how-it-works)
- [OpenTelemetry: The Bridge](#opentelemetry-the-bridge)
- [Trace Hierarchy and Data Captured](#trace-hierarchy-and-data-captured)
- [Authentication Flow](#authentication-flow)
- [Deployment Modes](#deployment-modes)
- [Configuration](#configuration)
- [The Agentic Loop in Detail](#the-agentic-loop-in-detail)
- [LLM Input/Output Per Step](#llm-inputoutput-per-step)
- [CloudWatch vs Langfuse Comparison](#cloudwatch-vs-langfuse-comparison)
- [Implementation Examples](#implementation-examples)
- [Troubleshooting](#troubleshooting)
- [References](#references)

---

## Overview

Langfuse is an open-source LLM observability platform that provides tracing, monitoring, and evaluation capabilities for AI agents. When integrated with Amazon Bedrock AgentCore, it offers:

- **Trace Visualization**: Hierarchical view of agent execution
- **Cost Tracking**: Detailed per-model token usage and pricing
- **LLM Playground**: Replay and test prompts directly
- **Evaluations**: LLM-as-a-judge automated evaluation
- **Prompt Management**: Version control for prompts
- **Session Analytics**: Group and analyze multi-turn conversations

### Integration Method

The integration uses **OpenTelemetry (OTEL)** as the transport layer. Strands Agents emit OTEL-format traces, which are sent to Langfuse's OTEL endpoint via HTTP.

```
Strands Agent ‚Üí OTEL Exporter ‚Üí Langfuse OTEL Endpoint ‚Üí Langfuse Dashboard
```

---

## Langfuse Hosting Options

LangfuseÎäî Îëê Í∞ÄÏßÄ Î∞©ÏãùÏúºÎ°ú Î∞∞Ìè¨Ìï† Ïàò ÏûàÏäµÎãàÎã§:

| ÏòµÏÖò | ÏÑ§Î™Ö | Ïû•Ï†ê | Îã®Ï†ê |
|------|------|------|------|
| **Public Cloud** | Langfuse Í¥ÄÎ¶¨Ìòï ÏÑúÎπÑÏä§ | Ï¶âÏãú ÏãúÏûë, Ïú†ÏßÄÎ≥¥Ïàò Î∂àÌïÑÏöî | Îç∞Ïù¥ÌÑ∞Í∞Ä Ïô∏Î∂Ä ÏÑúÎ≤ÑÏóê Ï†ÄÏû• |
| **Self-hosted Fargate** | AWS ECSÏóê ÏßÅÏ†ë Î∞∞Ìè¨ | Îç∞Ïù¥ÌÑ∞ ÏôÑÏ†Ñ Ï†úÏñ¥, VPC ÎÇ¥Î∂Ä Ïö¥ÏòÅ | Ïù∏ÌîÑÎùº Í¥ÄÎ¶¨ ÌïÑÏöî |

### Option 1: Public Langfuse Cloud

Í∞ÄÏû• Îπ†Î•¥Í≤å ÏãúÏûëÌï† Ïàò ÏûàÎäî Î∞©Î≤ïÏûÖÎãàÎã§. Free tierÍ∞Ä Ï†úÍ≥µÎê©ÎãàÎã§.

**ÏÑ§Ï†ï Î∞©Î≤ï:**
1. [langfuse.com](https://langfuse.com) ÏóêÏÑú Í≥ÑÏ†ï ÏÉùÏÑ±
2. ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ±
3. Settings ‚Üí API KeysÏóêÏÑú ÌÇ§ Î∞úÍ∏â
4. `.env` ÌååÏùº ÏÑ§Ï†ï:
   ```bash
   LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key
   LANGFUSE_SECRET_KEY=sk-lf-your-secret-key
   LANGFUSE_BASE_URL=https://us.cloud.langfuse.com  # US region
   # ÎòêÎäî https://cloud.langfuse.com  # EU region
   ```

**Endpoints:**
| Region | Base URL |
|--------|----------|
| US | `https://us.cloud.langfuse.com` |
| EU | `https://cloud.langfuse.com` |

### Option 2: Self-hosted on AWS Fargate

Îç∞Ïù¥ÌÑ∞Î•º AWS ÎÇ¥Î∂ÄÏóêÏÑú ÏôÑÏ†ÑÌûà Ï†úÏñ¥ÌïòÎ†§Î©¥ ECS FargateÏóê LangfuseÎ•º ÏßÅÏ†ë Î∞∞Ìè¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.

**Î∞∞Ìè¨ Í∞ÄÏù¥Îìú:** [deploy-langfuse-on-ecs-with-fargate](https://github.com/gonsoomoon-ml/deploy-langfuse-on-ecs-with-fargate)

**ÏïÑÌÇ§ÌÖçÏ≤ò:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        AWS VPC                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ     ALB     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ECS Fargate ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Aurora    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (Public)   ‚îÇ    ‚îÇ  (Langfuse)  ‚îÇ    ‚îÇ PostgreSQL  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ         ‚îÇ                                                    ‚îÇ
‚îÇ         ‚îÇ HTTP/HTTPS                                         ‚îÇ
‚îÇ         ‚ñº                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ              AgentCore Runtime                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         (OTEL traces ‚Üí Langfuse ALB)                ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**ÏÑ§Ï†ï Î∞©Î≤ï:**
1. GitHub Ï†ÄÏû•ÏÜå ÌÅ¥Î°† Î∞è CDK Î∞∞Ìè¨
2. Î∞∞Ìè¨ ÏôÑÎ£å ÌõÑ ALB DNS ÌôïÏù∏
3. Langfuse Ïõπ UI (ALB Ï£ºÏÜå)ÏóêÏÑú ÌîÑÎ°úÏ†ùÌä∏ Î∞è API ÌÇ§ ÏÉùÏÑ±
4. `.env` ÌååÏùº ÏÑ§Ï†ï:
   ```bash
   LANGFUSE_PUBLIC_KEY=pk-lf-your-key
   LANGFUSE_SECRET_KEY=sk-lf-your-key
   LANGFUSE_BASE_URL=http://your-alb-endpoint.region.elb.amazonaws.com
   ```

**Self-hosted Ïû•Ï†ê:**
- Îç∞Ïù¥ÌÑ∞Í∞Ä AWS VPC ÎÇ¥Î∂ÄÏóêÎßå Ï†ÄÏû•
- Í∏∞ÏóÖ Î≥¥Ïïà Ï†ïÏ±Ö Ï§ÄÏàò Ïö©Ïù¥
- ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÄÏó∞ ÏãúÍ∞Ñ ÏµúÏÜåÌôî
- Ïª§Ïä§ÌÖÄ ÎèÑÎ©îÏù∏ Î∞è SSL ÏÑ§Ï†ï Í∞ÄÎä•

---

## Architecture: How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Your Agent Code                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ LLM Call    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Tool Call   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ LLM Call    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ (Claude)    ‚îÇ    ‚îÇ (check_     ‚îÇ    ‚îÇ (Final      ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ    ‚îÇ  return)    ‚îÇ    ‚îÇ  Response)  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                      ‚îÇ
‚îÇ         ‚ñº                  ‚ñº                  ‚ñº                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ              Strands Telemetry (Auto-Instrumentation)        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   - Captures: inputs, outputs, tokens, latency, errors      ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                              ‚îÇ                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ OTEL Protocol (HTTP/gRPC)
                               ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   OTEL Exporter     ‚îÇ
                    ‚îÇ (OTLP over HTTPS)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚îÇ Authorization: Basic {base64(pk:sk)}
                               ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Langfuse        ‚îÇ
                    ‚îÇ  /api/public/otel   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## OpenTelemetry: The Bridge

OpenTelemetry (OTEL) is a CNCF standard for distributed tracing. It serves as the bridge between your agent and Langfuse.

### Key Concepts

| Concept | Description | Langfuse Mapping |
|---------|-------------|------------------|
| **Trace** | A complete request/response cycle | Langfuse Trace |
| **Span** | An individual operation within a trace | Langfuse Observation |
| **Event** | A point-in-time occurrence within a span | Langfuse Event |
| **Attribute** | Key-value metadata attached to spans | Langfuse Metadata |

### How Strands Telemetry Works

When you call `StrandsTelemetry().setup_otlp_exporter()`, it:

1. Initializes the OTEL SDK with the configured exporter
2. Instruments the Strands Agent SDK automatically
3. Captures all LLM calls, tool executions, and agent lifecycle events
4. Batches and sends traces to the configured endpoint

```python
from strands.telemetry import StrandsTelemetry

# This single line enables all telemetry
strands_telemetry = StrandsTelemetry().setup_otlp_exporter()
```

---

## Trace Hierarchy and Data Captured

### Trace Structure

```
Trace (one per agent invocation)
‚îÇ
‚îú‚îÄ‚îÄ Span: agent.invoke
‚îÇ   ‚îú‚îÄ‚îÄ Attribute: user.id = "customer@example.com"
‚îÇ   ‚îú‚îÄ‚îÄ Attribute: session.id = "session-123"
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Span: llm.chat (1st LLM call)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Event: gen_ai.system.message (system prompt)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Event: gen_ai.user.message (user input)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Event: gen_ai.choice (tool_use decision)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Attribute: gen_ai.usage.input_tokens = 1500
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Attribute: gen_ai.usage.output_tokens = 200
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Attribute: gen_ai.response.model = "claude-3-7-sonnet"
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Span: tool.execute (tool call)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Attribute: tool.name = "check_return_eligibility"
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Attribute: tool.input = {"order_id": "ORD-123"}
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Attribute: tool.output = {"eligible": true, ...}
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Attribute: duration_ms = 150
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Span: llm.chat (2nd LLM call - final response)
‚îÇ       ‚îú‚îÄ‚îÄ Event: gen_ai.tool.message (tool result)
‚îÇ       ‚îú‚îÄ‚îÄ Event: gen_ai.choice (end_turn)
‚îÇ       ‚îî‚îÄ‚îÄ Attribute: gen_ai.usage.output_tokens = 350
```

### Data Captured at Each Level

| Level | Data | Purpose |
|-------|------|---------|
| **Trace** | trace_id, session_id, user_id, tags | Group related operations |
| **Span** | name, start_time, end_time, status | Measure individual operations |
| **Event** | gen_ai.* events with content | Capture message content |
| **Attribute** | tokens, model, latency, metadata | Metrics and context |

### Event Types (gen_ai.* namespace)

| Event | Description |
|-------|-------------|
| `gen_ai.system.message` | System prompt content |
| `gen_ai.user.message` | User input message |
| `gen_ai.assistant.message` | LLM response (including tool calls) |
| `gen_ai.tool.message` | Tool execution result |
| `gen_ai.choice` | LLM decision (`tool_use` or `end_turn`) |
| `strands.telemetry.tracer` | Strands agent internal events |

---

## Authentication Flow

Langfuse uses **Basic Auth** over the OTEL HTTP endpoint.

### Step-by-Step

```python
# Step 1: Get credentials from Langfuse project settings
LANGFUSE_PUBLIC_KEY = "pk-lf-xxxxxxxx"
LANGFUSE_SECRET_KEY = "sk-lf-xxxxxxxx"

# Step 2: Create Basic Auth token
import base64
auth_string = f"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}"
auth_token = base64.b64encode(auth_string.encode()).decode()
# Result: "cGstbGYteHh4eHh4eHg6c2stbGYteHh4eHh4eHg="

# Step 3: Set OTEL exporter headers
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://us.cloud.langfuse.com/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {auth_token}"
```

### HTTP Request Format

When traces are sent to Langfuse:

```
POST https://us.cloud.langfuse.com/api/public/otel/v1/traces
Headers:
  Authorization: Basic cGstbGYteHh4eHh4eHg6c2stbGYteHh4eHh4eHg=
  Content-Type: application/x-protobuf
Body: [OTEL trace data in protobuf format]
```

### Langfuse Endpoints

| Region | Endpoint |
|--------|----------|
| US | `https://us.cloud.langfuse.com/api/public/otel` |
| EU | `https://cloud.langfuse.com/api/public/otel` |
| Self-hosted | `https://your-domain.com/api/public/otel` |

---

## Deployment Modes

### Mode A: Local Development (Direct OTEL)

Use this mode when running agents locally for development and testing.

```python
import os
import base64
from strands import Agent
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

# Configure Langfuse credentials
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..."
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
os.environ["LANGFUSE_BASE_URL"] = "https://us.cloud.langfuse.com"

LANGFUSE_AUTH = base64.b64encode(
    f"{os.environ['LANGFUSE_PUBLIC_KEY']}:{os.environ['LANGFUSE_SECRET_KEY']}".encode()
).decode()

os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = f"{os.environ['LANGFUSE_BASE_URL']}/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

# Initialize telemetry - sends directly to Langfuse
StrandsTelemetry().setup_otlp_exporter()

model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")

agent = Agent(
    model=model,
    system_prompt="You are a helpful assistant.",
    trace_attributes={
        "session.id": "local-test-123",
        "user.id": "developer@example.com",
        "langfuse.tags": ["development", "test"]
    }
)

# Traces go to Langfuse immediately
response = agent("Hello, how can you help me?")
```

**Data Flow:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your Code   ‚îÇ‚îÄ‚îÄOTEL‚îÄ‚îÄ‚ñ∂‚îÇ   Langfuse   ‚îÇ
‚îÇ  (Local)     ‚îÇ         ‚îÇ   Cloud      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Mode B: AgentCore Runtime (Container Environment)

Use this mode when deploying agents to AgentCore Runtime in production.

**Entrypoint file (e.g., `agent_entrypoint.py`):**

```python
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from strands import Agent
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

app = BedrockAgentCoreApp()

def initialize_agent():
    """Initialize agent with telemetry from environment variables."""
    # Telemetry reads OTEL_* env vars set during launch()
    StrandsTelemetry().setup_otlp_exporter()

    model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")

    agent = Agent(
        model=model,
        system_prompt="You are a helpful assistant.",
        tools=[...]
    )
    return agent

@app.entrypoint
async def invoke(payload):
    """Streaming entrypoint for AgentCore Runtime."""
    agent = initialize_agent()
    user_input = payload.get("prompt", "")

    async for event in agent.stream_async(user_input):
        yield event

if __name__ == "__main__":
    app.run()
```

**Deployment notebook:**

```python
import base64
from bedrock_agentcore_starter_toolkit import Runtime

# Langfuse configuration
langfuse_public_key = "pk-lf-..."
langfuse_secret_key = "sk-lf-..."
langfuse_auth = base64.b64encode(
    f"{langfuse_public_key}:{langfuse_secret_key}".encode()
).decode()

otel_endpoint = "https://us.cloud.langfuse.com/api/public/otel"
otel_headers = f"Authorization=Basic {langfuse_auth}"

# Configure runtime
runtime = Runtime()
runtime.configure(
    entrypoint="agent_entrypoint.py",
    execution_role=execution_role_arn,
    agent_name="my-agent-with-langfuse",
    region=region,
    disable_otel=True,  # Disable CloudWatch OTEL, use Langfuse instead
)

# Launch with Langfuse environment variables
runtime.launch(
    env_vars={
        "OTEL_EXPORTER_OTLP_ENDPOINT": otel_endpoint,
        "OTEL_EXPORTER_OTLP_HEADERS": otel_headers,
        "DISABLE_ADOT_OBSERVABILITY": "true"
    }
)
```

**Data Flow:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   AgentCore Runtime                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Container   ‚îÇ    ‚îÇ   Strands    ‚îÇ    ‚îÇ    OTEL      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (ECR)       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Agent      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Exporter   ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ Langfuse
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚ñ≤                                                    ‚îÇ
‚îÇ         ‚îÇ env_vars from launch()                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Configuration

### Environment Variables

| Variable | Purpose | Example |
|----------|---------|---------|
| `LANGFUSE_PUBLIC_KEY` | Langfuse public API key | `pk-lf-xxxxxxxx` |
| `LANGFUSE_SECRET_KEY` | Langfuse secret API key | `sk-lf-xxxxxxxx` |
| `LANGFUSE_BASE_URL` | Langfuse instance URL | `https://us.cloud.langfuse.com` |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | OTEL exporter endpoint | `https://us.cloud.langfuse.com/api/public/otel` |
| `OTEL_EXPORTER_OTLP_HEADERS` | OTEL auth headers | `Authorization=Basic xxx...` |
| `OTEL_SERVICE_NAME` | Service name in traces | `ecommerce-agent` |
| `DISABLE_ADOT_OBSERVABILITY` | Disable AWS ADOT | `true` |

### Trace Attributes

Configure trace attributes in the Agent constructor to organize data in Langfuse:

```python
agent = Agent(
    model=model,
    trace_attributes={
        # Required for Langfuse grouping
        "session.id": "unique-session-id",      # Groups multi-turn conversations
        "user.id": "customer@example.com",      # User-level analytics

        # Optional Langfuse-specific
        "langfuse.tags": ["production", "kr"],  # Filterable tags
        "langfuse.metadata": {                  # Custom metadata
            "customer_tier": "premium",
            "order_value": 150000
        }
    }
)
```

### Installation

```bash
# Install required packages
pip install strands-agents[otel] langfuse

# Or with UV
uv add strands-agents[otel] langfuse
```

The `[otel]` extra is required to enable OpenTelemetry instrumentation.

---

## The Agentic Loop in Detail

When your agent processes a request, here's what gets traced:

```
Request: "Î∞òÌíàÌïòÍ≥† Ïã∂Ïñ¥Ïöî" (I want to return something)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Trace: agent-invocation-abc123                                       ‚îÇ
‚îÇ Session: lab5-session-xyz                                           ‚îÇ
‚îÇ User: customer@example.com                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ [0ms] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ       ‚îÇ                                                              ‚îÇ
‚îÇ       ‚ñº                                                              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Span: llm.chat.completion (1200ms)                              ‚îÇ ‚îÇ
‚îÇ ‚îÇ Model: us.anthropic.claude-3-7-sonnet                           ‚îÇ ‚îÇ
‚îÇ ‚îÇ Input Tokens: 1,847  |  Output Tokens: 156                      ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                                  ‚îÇ ‚îÇ
‚îÇ ‚îÇ Events:                                                          ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚Ä¢ gen_ai.system.message: "ÎãπÏã†ÏùÄ K-Style Ïù¥Ïª§Î®∏Ïä§..."          ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚Ä¢ gen_ai.user.message: "Î∞òÌíàÌïòÍ≥† Ïã∂Ïñ¥Ïöî"                       ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚Ä¢ gen_ai.choice: tool_use ‚Üí check_return_eligibility          ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ                                                              ‚îÇ
‚îÇ       ‚ñº                                                              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Span: tool.check_return_eligibility (89ms)                      ‚îÇ ‚îÇ
‚îÇ ‚îÇ Input: {"order_id": "ORD-20240115-TEST"}                        ‚îÇ ‚îÇ
‚îÇ ‚îÇ Output: {"eligible": true, "reason": "14Ïùº Ïù¥ÎÇ¥", ...}          ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚îÇ                                                              ‚îÇ
‚îÇ       ‚ñº                                                              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Span: llm.chat.completion (1850ms)                              ‚îÇ ‚îÇ
‚îÇ ‚îÇ Model: us.anthropic.claude-3-7-sonnet                           ‚îÇ ‚îÇ
‚îÇ ‚îÇ Input Tokens: 2,103  |  Output Tokens: 287                      ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                                  ‚îÇ ‚îÇ
‚îÇ ‚îÇ Events:                                                          ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚Ä¢ gen_ai.tool.message: {"eligible": true, ...}                ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚Ä¢ gen_ai.choice: end_turn                                     ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚Ä¢ gen_ai.assistant.message: "ÎÑ§, Î∞òÌíà Í∞ÄÎä•Ìï©ÎãàÎã§..."           ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ [3139ms] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ Total: 3.14s | Tokens: 3,950 in / 443 out | Cost: $0.0156          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### What Langfuse Shows

Once Langfuse receives the OTEL traces, it provides:

| Feature | How It Uses Trace Data |
|---------|------------------------|
| **Trace Timeline** | Visualizes spans hierarchically with timing |
| **Cost Calculation** | Uses `input_tokens` + `output_tokens` + model pricing |
| **Latency Analysis** | Aggregates span durations by operation type |
| **Session Grouping** | Groups traces by `session.id` attribute |
| **User Analytics** | Tracks usage per `user.id` |
| **Error Debugging** | Shows span status and error messages |
| **LLM Playground** | Replays prompts using captured messages |
| **Evaluations** | Runs LLM-as-a-judge on captured inputs/outputs |

---

## LLM Input/Output Per Step

This section shows the actual message content that flows into and out of the LLM at each step of the agentic loop.

### Step 1: First LLM Call (User Request ‚Üí Tool Decision)

**INPUT to LLM:**

```json
{
  "messages": [
    {
      "role": "system",
      "content": "ÎãπÏã†ÏùÄ K-Style Ïù¥Ïª§Î®∏Ïä§ Í≥†Í∞ù ÏßÄÏõê ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§.\n\nÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨:\n- check_return_eligibility: Ï£ºÎ¨∏Ïùò Î∞òÌíà Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏\n- get_product_recommendations: ÏÉÅÌíà Ï∂îÏ≤ú\n- process_return_request: Î∞òÌíà Ï≤òÎ¶¨\n\nÍ≥†Í∞ùÏóêÍ≤å ÏπúÏ†àÌïòÍ≥† ÎèÑÏõÄÏù¥ ÎêòÎäî ÏùëÎãµÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî."
    },
    {
      "role": "user",
      "content": "Ï£ºÎ¨∏Î≤àÌò∏ ORD-20240115-TESTÏùò 'ÌîåÎùºÏõå Ìå®ÌÑ¥ ÏõêÌîºÏä§' Î∞òÌíàÏù¥ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?"
    }
  ],
  "tools": [
    {
      "name": "check_return_eligibility",
      "description": "Check if an order is eligible for return",
      "input_schema": {
        "type": "object",
        "properties": {
          "order_id": {"type": "string"}
        },
        "required": ["order_id"]
      }
    }
  ]
}
```

**OUTPUT from LLM:**

```json
{
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "ÎÑ§, Ï£ºÎ¨∏Î≤àÌò∏ ORD-20240115-TESTÏùò Î∞òÌíà Í∞ÄÎä• Ïó¨Î∂ÄÎ•º ÌôïÏù∏Ìï¥ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§."
    },
    {
      "type": "tool_use",
      "id": "toolu_01ABC123",
      "name": "check_return_eligibility",
      "input": {
        "order_id": "ORD-20240115-TEST"
      }
    }
  ],
  "stop_reason": "tool_use",
  "usage": {
    "input_tokens": 1500,
    "output_tokens": 156
  }
}
```

### Step 2: Tool Execution (Agent Runtime, not LLM)

The agent runtime executes the tool function locally. This is NOT an LLM call.

**Tool Input:**

```json
{
  "order_id": "ORD-20240115-TEST"
}
```

**Tool Output:**

```json
{
  "order_id": "ORD-20240115-TEST",
  "product_name": "ÌîåÎùºÏõå Ìå®ÌÑ¥ ÏõêÌîºÏä§",
  "eligible": true,
  "reason": "Íµ¨Îß§ ÌõÑ 14Ïùº Ïù¥ÎÇ¥",
  "purchase_date": "2024-01-15",
  "return_deadline": "2024-01-29",
  "refund_amount": 89000,
  "return_method": "Î¨¥Î£å Î∞òÌíà ÌîΩÏóÖ"
}
```

### Step 3: Second LLM Call (Tool Result ‚Üí Final Response)

**INPUT to LLM:**

Note how all previous messages are included (conversation history accumulates).

```json
{
  "messages": [
    {
      "role": "system",
      "content": "ÎãπÏã†ÏùÄ K-Style Ïù¥Ïª§Î®∏Ïä§ Í≥†Í∞ù ÏßÄÏõê ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§..."
    },
    {
      "role": "user",
      "content": "Ï£ºÎ¨∏Î≤àÌò∏ ORD-20240115-TESTÏùò 'ÌîåÎùºÏõå Ìå®ÌÑ¥ ÏõêÌîºÏä§' Î∞òÌíàÏù¥ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?"
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "ÎÑ§, Ï£ºÎ¨∏Î≤àÌò∏ ORD-20240115-TESTÏùò Î∞òÌíà Í∞ÄÎä• Ïó¨Î∂ÄÎ•º ÌôïÏù∏Ìï¥ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§."
        },
        {
          "type": "tool_use",
          "id": "toolu_01ABC123",
          "name": "check_return_eligibility",
          "input": {"order_id": "ORD-20240115-TEST"}
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01ABC123",
          "content": "{\"order_id\": \"ORD-20240115-TEST\", \"product_name\": \"ÌîåÎùºÏõå Ìå®ÌÑ¥ ÏõêÌîºÏä§\", \"eligible\": true, \"reason\": \"Íµ¨Îß§ ÌõÑ 14Ïùº Ïù¥ÎÇ¥\", \"purchase_date\": \"2024-01-15\", \"return_deadline\": \"2024-01-29\", \"refund_amount\": 89000, \"return_method\": \"Î¨¥Î£å Î∞òÌíà ÌîΩÏóÖ\"}"
        }
      ]
    }
  ]
}
```

**OUTPUT from LLM:**

```json
{
  "role": "assistant",
  "content": [
    {
      "type": "text",
      "text": "ÎÑ§, Ï£ºÎ¨∏Î≤àÌò∏ ORD-20240115-TESTÏùò 'ÌîåÎùºÏõå Ìå®ÌÑ¥ ÏõêÌîºÏä§' Î∞òÌíàÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§!\n\nüìã **Î∞òÌíà Ï†ïÎ≥¥**\n- **Î∞òÌíà Í∞ÄÎä• Ïó¨Î∂Ä**: Í∞ÄÎä•\n- **ÏÇ¨Ïú†**: Íµ¨Îß§ ÌõÑ 14Ïùº Ïù¥ÎÇ¥ (Íµ¨Îß§Ïùº: 2024-01-15)\n- **Î∞òÌíà Í∏∞Ìïú**: 2024ÎÖÑ 1Ïõî 29ÏùºÍπåÏßÄ\n- **ÌôòÎ∂à Í∏àÏï°**: 89,000Ïõê\n- **Î∞òÌíà Î∞©Î≤ï**: Î¨¥Î£å Î∞òÌíà ÌîΩÏóÖ\n\nÎ∞òÌíàÏùÑ ÏßÑÌñâÌïòÏãúÍ≤†ÏäµÎãàÍπå? ÏßÑÌñâÏùÑ ÏõêÌïòÏãúÎ©¥ ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî. Î¨¥Î£å ÌîΩÏóÖ ÏÑúÎπÑÏä§Î•º ÏòàÏïΩÌï¥ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§."
    }
  ],
  "stop_reason": "end_turn",
  "usage": {
    "input_tokens": 2103,
    "output_tokens": 287
  }
}
```

### Visual Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              AGENTIC LOOP                                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                  ‚îÇ
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó  ‚îÇ
‚îÇ  ‚ïë  STEP 1: First LLM Call                                                   ‚ïë  ‚îÇ
‚îÇ  ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£  ‚îÇ
‚îÇ  ‚ïë                                                                           ‚ïë  ‚îÇ
‚îÇ  ‚ïë  INPUT                              OUTPUT                                ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ System: K-Style     ‚îÇ           ‚îÇ Text: "ÌôïÏù∏Ìï¥       ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ Í≥†Í∞ù ÏßÄÏõê ÏóêÏù¥Ï†ÑÌä∏... ‚îÇ    ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇ ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§"       ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ User: "Î∞òÌíàÏù¥       ‚îÇ           ‚îÇ Tool Use:           ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ Í∞ÄÎä•ÌïúÍ∞ÄÏöî?"        ‚îÇ           ‚îÇ check_return_       ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îÇ eligibility         ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ Tools: [check_      ‚îÇ           ‚îÇ {order_id: "ORD-"}  ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ return_eligibility] ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           stop_reason: "tool_use"               ‚ïë  ‚îÇ
‚îÇ  ‚ïë                                                                           ‚ïë  ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚îÇ
‚îÇ                                     ‚îÇ                                            ‚îÇ
‚îÇ                                     ‚ñº                                            ‚îÇ
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó  ‚îÇ
‚îÇ  ‚ïë  STEP 2: Tool Execution (Agent Runtime - NOT LLM)                         ‚ïë  ‚îÇ
‚îÇ  ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£  ‚îÇ
‚îÇ  ‚ïë                                                                           ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ Tool Input:         ‚îÇ           ‚îÇ Tool Output:        ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ {                   ‚îÇ    ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇ {                   ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ   order_id:         ‚îÇ  (DB/API) ‚îÇ   eligible: true,   ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ   "ORD-20240115"    ‚îÇ           ‚îÇ   refund: 89000,    ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ }                   ‚îÇ           ‚îÇ   deadline: "1/29"  ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ }                   ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚ïë  ‚îÇ
‚îÇ  ‚ïë                                                                           ‚ïë  ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚îÇ
‚îÇ                                     ‚îÇ                                            ‚îÇ
‚îÇ                                     ‚ñº                                            ‚îÇ
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó  ‚îÇ
‚îÇ  ‚ïë  STEP 3: Second LLM Call                                                  ‚ïë  ‚îÇ
‚îÇ  ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£  ‚îÇ
‚îÇ  ‚ïë                                                                           ‚ïë  ‚îÇ
‚îÇ  ‚ïë  INPUT                              OUTPUT                                ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ System: (same)      ‚îÇ           ‚îÇ Text:               ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îÇ "ÎÑ§, Î∞òÌíàÏù¥         ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ User: "Î∞òÌíà Í∞ÄÎä•?"  ‚îÇ    ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇ Í∞ÄÎä•Ìï©ÎãàÎã§!         ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îÇ                     ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ Assistant: tool_use ‚îÇ           ‚îÇ üìã Î∞òÌíà Ï†ïÎ≥¥        ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îÇ - ÌôòÎ∂à: 89,000Ïõê    ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ User (tool_result): ‚îÇ           ‚îÇ - Í∏∞Ìïú: 1/29        ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îÇ {eligible: true...} ‚îÇ           ‚îÇ - Î¨¥Î£å ÌîΩÏóÖ Í∞ÄÎä•"   ‚îÇ               ‚ïë  ‚îÇ
‚îÇ  ‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚ïë  ‚îÇ
‚îÇ  ‚ïë                                    stop_reason: "end_turn"               ‚ïë  ‚îÇ
‚îÇ  ‚ïë                                                                           ‚ïë  ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Message Accumulation Pattern

The key insight is that **messages accumulate** with each LLM call:

| LLM Call | Messages in Context |
|----------|---------------------|
| **1st** | `[system, user]` |
| **2nd** | `[system, user, assistant(tool_use), user(tool_result)]` |
| **3rd** (if needed) | `[system, user, assistant, user(tool_result), assistant(tool_use_2), user(tool_result_2)]` |

This is why `input_tokens` increases with each call:
- 1st call: 1,500 tokens (system + user)
- 2nd call: 2,103 tokens (system + user + assistant + tool_result)

### Multi-Tool Example

If the agent needs multiple tools, the loop continues:

```
Step 1: LLM ‚Üí tool_use (tool A)
Step 2: Execute tool A ‚Üí result A
Step 3: LLM ‚Üí tool_use (tool B)  ‚Üê receives result A, decides to call another tool
Step 4: Execute tool B ‚Üí result B
Step 5: LLM ‚Üí end_turn           ‚Üê receives both results, generates final response
```

**Message accumulation for multi-tool:**

```json
{
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "original question"},
    {"role": "assistant", "content": [{"type": "tool_use", "name": "tool_A", ...}]},
    {"role": "user", "content": [{"type": "tool_result", "tool_use_id": "A", ...}]},
    {"role": "assistant", "content": [{"type": "tool_use", "name": "tool_B", ...}]},
    {"role": "user", "content": [{"type": "tool_result", "tool_use_id": "B", ...}]}
  ]
}
```

---

## CloudWatch vs Langfuse Comparison

### Feature Comparison

| Feature | CloudWatch GenAI Observability | Langfuse |
|---------|-------------------------------|----------|
| **Setup** | Automatic (default in AgentCore) | Manual configuration required |
| **Cost Tracking** | Basic | Detailed per-model pricing |
| **LLM Playground** | No | Yes (test prompts directly) |
| **Evaluations** | Manual | LLM-as-a-judge built-in |
| **Prompt Management** | No | Version control for prompts |
| **Open Source** | No | Yes (self-host option) |
| **AWS Native** | Yes | Partner integration |
| **X-Ray Integration** | Yes | No |
| **Logs Insights** | Yes | Limited |

### Data Flow Comparison

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ            AgentCore Runtime                     ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
                    ‚îÇ  ‚îÇ          Strands Agent                   ‚îÇ    ‚îÇ
                    ‚îÇ  ‚îÇ    (with OTEL instrumentation)          ‚îÇ    ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                    ‚îÇ                     ‚îÇ                            ‚îÇ
                    ‚îÇ                     ‚îÇ OTEL traces                ‚îÇ
                    ‚îÇ                     ‚ñº                            ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
                    ‚îÇ  ‚îÇ         OTEL Collector/Exporter          ‚îÇ    ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                    ‚îÇ              ‚îÇ                  ‚îÇ                ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ                  ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ (Default: AWS ADOT)                      ‚îÇ (Custom: Langfuse)    ‚îÇ
           ‚ñº                                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CloudWatch GenAI      ‚îÇ                ‚îÇ       Langfuse          ‚îÇ
‚îÇ   Observability         ‚îÇ                ‚îÇ                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ AWS native            ‚îÇ                ‚îÇ ‚Ä¢ Open source           ‚îÇ
‚îÇ ‚Ä¢ Auto-enabled          ‚îÇ                ‚îÇ ‚Ä¢ LLM Playground        ‚îÇ
‚îÇ ‚Ä¢ X-Ray integration     ‚îÇ                ‚îÇ ‚Ä¢ Prompt versioning     ‚îÇ
‚îÇ ‚Ä¢ Basic dashboards      ‚îÇ                ‚îÇ ‚Ä¢ LLM-as-judge evals    ‚îÇ
‚îÇ ‚Ä¢ Logs Insights queries ‚îÇ                ‚îÇ ‚Ä¢ Rich UI               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### When to Use Which

| Use Case | Recommendation |
|----------|----------------|
| AWS-only infrastructure, minimal setup | CloudWatch |
| Need prompt versioning and A/B testing | Langfuse |
| Want LLM-as-a-judge evaluations | Langfuse |
| Require X-Ray distributed tracing | CloudWatch |
| Open source / self-hosted requirement | Langfuse |
| Multi-cloud or hybrid deployments | Langfuse |

---

## Implementation Examples

### Example 1: Local Development with Langfuse

```python
import os
import base64
from strands import Agent
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

# Configure Langfuse
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..."
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
LANGFUSE_AUTH = base64.b64encode(
    f"{os.environ['LANGFUSE_PUBLIC_KEY']}:{os.environ['LANGFUSE_SECRET_KEY']}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://us.cloud.langfuse.com/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

# Initialize telemetry
StrandsTelemetry().setup_otlp_exporter()

# Create agent
model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")
agent = Agent(
    model=model,
    system_prompt="You are a helpful e-commerce assistant.",
    trace_attributes={
        "session.id": "dev-session-001",
        "user.id": "developer@example.com"
    }
)

# Run agent
response = agent("What products do you have?")
print(response)
```

### Example 2: AgentCore Runtime with Langfuse

**File: `langfuse_agent.py`**

```python
import os
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from strands import Agent, tool
from strands.telemetry import StrandsTelemetry
from strands.models.bedrock import BedrockModel

app = BedrockAgentCoreApp()

@tool
def check_return_eligibility(order_id: str) -> dict:
    """Check if an order is eligible for return."""
    return {
        "order_id": order_id,
        "eligible": True,
        "reason": "Within 14-day return window"
    }

def create_agent():
    StrandsTelemetry().setup_otlp_exporter()

    model = BedrockModel(
        model_id=os.getenv("BEDROCK_MODEL_ID", "us.anthropic.claude-3-7-sonnet-20250219-v1:0")
    )

    return Agent(
        model=model,
        system_prompt="You are a K-Style e-commerce customer support agent.",
        tools=[check_return_eligibility]
    )

@app.entrypoint
async def invoke(payload):
    agent = create_agent()
    user_input = payload.get("prompt", "")
    async for event in agent.stream_async(user_input):
        yield event

if __name__ == "__main__":
    app.run()
```

**Deployment:**

```python
import base64
from bedrock_agentcore_starter_toolkit import Runtime

# Langfuse credentials
langfuse_pk = "pk-lf-..."
langfuse_sk = "sk-lf-..."
langfuse_auth = base64.b64encode(f"{langfuse_pk}:{langfuse_sk}".encode()).decode()

runtime = Runtime()
runtime.configure(
    entrypoint="langfuse_agent.py",
    execution_role=execution_role_arn,
    agent_name="ecommerce-agent-langfuse",
    region=region,
    disable_otel=True,
)

runtime.launch(
    env_vars={
        "BEDROCK_MODEL_ID": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "OTEL_EXPORTER_OTLP_ENDPOINT": "https://us.cloud.langfuse.com/api/public/otel",
        "OTEL_EXPORTER_OTLP_HEADERS": f"Authorization=Basic {langfuse_auth}",
        "DISABLE_ADOT_OBSERVABILITY": "true"
    }
)
```

### Example 3: Using Langfuse Python SDK with Strands

For more advanced use cases, you can combine Strands telemetry with Langfuse's Python SDK:

```python
from langfuse import observe, propagate_attributes, get_client
from strands import Agent
from strands.telemetry import StrandsTelemetry

StrandsTelemetry().setup_otlp_exporter()

@observe()
def process_customer_request(user_input: str, customer_id: str):
    """Process a customer request with additional Langfuse tracking."""
    with propagate_attributes(
        user_id=customer_id,
        session_id=f"session-{customer_id}",
        tags=["customer-support"],
        metadata={"source": "web-chat"}
    ):
        agent = Agent(model=model, tools=[...])
        result = agent(user_input)

        # Update trace with custom data
        langfuse = get_client()
        langfuse.update_current_trace(
            input=user_input,
            output=result,
            metadata={"customer_id": customer_id}
        )

        return result

langfuse.flush()  # Ensure all traces are sent
```

---

## Troubleshooting

### Error: Traces not appearing in Langfuse

**Possible causes:**

1. **Missing OTEL extra**: Ensure you installed with `[otel]`
   ```bash
   pip install strands-agents[otel]
   ```

2. **Incorrect credentials**: Verify your public/secret keys
   ```python
   # Test authentication
   import requests
   response = requests.get(
       "https://us.cloud.langfuse.com/api/public/health",
       headers={"Authorization": f"Basic {langfuse_auth}"}
   )
   print(response.status_code)  # Should be 200
   ```

3. **Telemetry not initialized**: Ensure `setup_otlp_exporter()` is called before agent creation

4. **Environment variables not set**: Check all required env vars are set
   ```python
   print(os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT"))
   print(os.environ.get("OTEL_EXPORTER_OTLP_HEADERS"))
   ```

### Error: ADOT and Langfuse conflict

**Cause**: Both CloudWatch ADOT and Langfuse OTEL exporters are active

**Solution**: Disable ADOT when using Langfuse
```python
runtime.configure(
    ...
    disable_otel=True,  # Disable AgentCore's built-in OTEL
)

runtime.launch(
    env_vars={
        "DISABLE_ADOT_OBSERVABILITY": "true",
        ...
    }
)
```

### Error: Missing trace attributes in Langfuse

**Cause**: `trace_attributes` not passed to Agent

**Solution**: Always include trace attributes
```python
agent = Agent(
    model=model,
    trace_attributes={
        "session.id": session_id,
        "user.id": user_id
    }
)
```

### Error: High latency in trace delivery

**Cause**: Traces are batched and sent periodically

**Solution**: For real-time debugging, flush manually
```python
from langfuse import get_client
langfuse = get_client()
langfuse.flush()
```

---

## References

### Official Documentation

- [Langfuse + Amazon Bedrock Integration](https://langfuse.com/integrations/model-providers/amazon-bedrock)
- [Langfuse + Amazon Bedrock AgentCore](https://langfuse.com/integrations/frameworks/amazon-agentcore)
- [Langfuse + Strands Agents](https://langfuse.com/integrations/frameworks/strands-agents)
- [AWS Blog: AgentCore Observability with Langfuse](https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-agentcore-observability-with-langfuse/)

### Sample Code

- [AgentCore Samples - Langfuse Notebook](https://github.com/awslabs/amazon-bedrock-agentcore-samples/blob/main/01-tutorials/06-AgentCore-observability/04-Agentcore-runtime-partner-observability/Langfuse/runtime_with_strands_and_langfuse.ipynb)
- [Strands Agents Observability Sample](https://github.com/strands-agents/samples/blob/main/01-tutorials/01-fundamentals/08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb)
- [Langfuse Docs - AWS Strands Agents Cookbook](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/integration_aws_strands_agents.ipynb)

### Self-hosted Deployment

- [Deploy Langfuse on ECS with Fargate](https://github.com/gonsoomoon-ml/deploy-langfuse-on-ecs-with-fargate) - AWS FargateÏóê Langfuse Î∞∞Ìè¨ Í∞ÄÏù¥Îìú

### Related Documentation

- [Amazon Bedrock AgentCore Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/)
- [OpenTelemetry Python SDK](https://opentelemetry.io/docs/languages/python/)
- [Langfuse OpenTelemetry Integration](https://langfuse.com/docs/integrations/opentelemetry)
